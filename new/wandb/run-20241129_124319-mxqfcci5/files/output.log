Training:   0%|                                                                     | 0/781 [00:00<?, ?it/s]/home/aileen/dl/new/train_model.py:53: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
/home/aileen/dl/new/train_model.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
Training:   1%|▍                                                          | 6/781 [00:32<1:08:11,  5.28s/it]Traceback (most recent call last):
  File "/home/aileen/dl/new/train_model.py", line 166, in <module>
    train_model(base_llm_model, mask,soft,  train_dataloader,  optimizer, scheduler, num_epochs, device, name, "/home/aileen/dl/new/models")
  File "/home/aileen/dl/new/train_model.py", line 93, in train_model
    scaler.scale(loss).backward()
  File "/home/aileen/anaconda3/envs/duo/lib/python3.10/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/home/aileen/anaconda3/envs/duo/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/aileen/anaconda3/envs/duo/lib/python3.10/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
